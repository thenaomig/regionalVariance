{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the definitive version for producing CMIP5 time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xy\n",
    "import string\n",
    "from copy import deepcopy\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "from constants import * #<= my defined global variables\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileInfo(dataDir,fileNameSplitter='_',fileNamePos=2):\n",
    "    def justDataFiles(path=None):\n",
    "        for f in os.listdir(path):\n",
    "            if ('p1_21' not in f) and ('p1_22' not in f) and ('.nc' in f):\n",
    "                yield f\n",
    "            \n",
    "    files = [x for x in justDataFiles(dataDir)]  \n",
    "    files.sort()\n",
    "    \n",
    "    #make a list of the model names in this file list\n",
    "    modelNames = []\n",
    "    for oneFile in files:\n",
    "        justFileName = str.split(oneFile,'/')[-1]\n",
    "        #print justFileName\n",
    "        justEnsMemNum = str.split(justFileName,fileNameSplitter)[fileNamePos]\n",
    "        #print justEnsMemNum\n",
    "        modelNames.append(justEnsMemNum)\n",
    "    modelNames = list(set(modelNames))\n",
    "\n",
    "    #make a dictionary of all the files that go with each model\n",
    "    keyvaluepairs = []\n",
    "    for model in modelNames:\n",
    "        uniqueModelName = ''.join([model,fileNameSplitter])\n",
    "        oneModelFileList = []\n",
    "        for oneFile in files:\n",
    "            if uniqueModelName in oneFile:\n",
    "                oneModelFileList.append(oneFile)\n",
    "        tupleOfKeyValue = (model,oneModelFileList)\n",
    "        keyvaluepairs.append(tupleOfKeyValue)\n",
    "\n",
    "    fileDict = dict(keyvaluepairs)\n",
    "    \n",
    "    return fileDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAreaAvg(data,bounds,weights,landFrac): #,t,weights):\n",
    "\n",
    "    weightedMean = 0.0\n",
    "    weightTotal = 0.0\n",
    "    \n",
    "    areaSubset = data.sel(lon=slice(bounds.lonMin,bounds.lonMax),lat=slice(bounds.latMin,bounds.latMax)) #.isel(time=t)\n",
    "    if landFrac and excludeOcean:\n",
    "        landFrac = landFrac.sel(lon=slice(bounds.lonMin,bounds.lonMax),lat=slice(bounds.latMin,bounds.latMax))\n",
    "        areaSubset = areaSubset.where(landFrac>0.5)\n",
    "        \n",
    "    useCosLat = False\n",
    "    if weights is None:\n",
    "        useCosLat = True\n",
    "    else:\n",
    "        try:\n",
    "            if len(weights.dims)==2:\n",
    "                weightsSubset = weights.sel(lon=slice(bounds.lonMin,bounds.lonMax),lat=slice(bounds.latMin,bounds.latMax))\n",
    "                weightedMean = (areaSubset * np.asarray(weightsSubset)).sum('lat').sum('lon')\n",
    "            elif len(weights.dims)==1:\n",
    "                weightsSubset = weights.sel(lat=slice(bounds.latMin,bounds.latMax))\n",
    "                weightedMean = (areaSubset.mean('lon') * np.asarray(weightsSubset)).sum('lat')\n",
    "        except:\n",
    "            #try *not* using the weights from the files, which might need to be updated\n",
    "            useCosLat = True\n",
    "            print \"an exception!\", weights.shape\n",
    "            \n",
    "    if useCosLat:\n",
    "        weightsSubset = np.cos(np.deg2rad(areaSubset.lat))\n",
    "        weightedMean = (areaSubset.mean('lon') * np.asarray(weightsSubset)).sum('lat')\n",
    "        \n",
    "    weightTotal = weightsSubset.sum()\n",
    "    return weightedMean/np.double(weightTotal)\n",
    "\n",
    "def convertPrecipUnits(dataVsTime, startingFrom=''): # converts to mm/day                                                                                                                       \n",
    "    if startingFrom == 'm/s':\n",
    "        return dataVsTime * 1000.0 * 60.0 * 60.0 * 24.0\n",
    "    else: #default for CMIP5                                                                                                                           \n",
    "        return dataVsTime * 60.0*60.0*24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOneFile(filename,field,bounds,weightFileName,landFracFileName):\n",
    "    \n",
    "    oneEnsMem = xy.open_dataset(filename)\n",
    "        \n",
    "    #get weights from weighta file if exists:\n",
    "    try:\n",
    "        weights = xy.open_dataset(weightFileName)\n",
    "        weights = weights.areacella\n",
    "    except:\n",
    "        weights = None\n",
    "        \n",
    "    if excludeOcean:\n",
    "        #get land fraction data if exists:\n",
    "        try:\n",
    "            landFrac = xy.open_dataset(landFracFileName)\n",
    "            landFrac = landFrac.sftlf\n",
    "        except:\n",
    "            landFrac = None\n",
    "    else:\n",
    "        landFrac = None\n",
    "            \n",
    "    #we're not looking out past 2100, which a few models provide, and anyway it would cause indexing errors\n",
    "    if ('historical' not in filename) and ('CESMens' not in filename):\n",
    "        if ('IPSL-CM5A-LR' in filename or 'CSIRO-Mk3L-1-2' in filename): #len(oneEnsMem.time) >= 2262:\n",
    "            oneEnsMem = oneEnsMem.isel(time=slice(0,1140)) #just until 2100-12 century\n",
    "            oneEnsMem['time'] = pd.date_range('2006-01-01',periods=1140,freq='MS')\n",
    "        oneEnsMem = oneEnsMem.sel(time=slice('2006-01-01','2099-12-31')) #a bit more exactly\n",
    "        #if(np.bool(oneEnsMem.time[-1] < np.datetime64('2099-11-30'))):\n",
    "            #print \"file doesn't go to end-of-century, only \", np.datetime_as_string(oneEnsMem.time[-1])\n",
    "    \n",
    "    timeSeries = getAreaAvg(oneEnsMem[field],bounds,weights,landFrac).to_series() \n",
    "    if field == 'tas':\n",
    "        if oneEnsMem[field].units != 'K':\n",
    "            print filename\n",
    "            print oneEnsMem[field].units\n",
    "        if np.double(oneEnsMem[field].mean()) < 200.0:\n",
    "            print filename\n",
    "            print oneEnsMem[field].units\n",
    "    \n",
    "    if field == 'pr':\n",
    "        timeSeries = convertPrecipUnits(timeSeries)\n",
    "    \n",
    "    try:    \n",
    "        timeSeries.index = timeSeries.index.to_period(freq='M') \n",
    "    except:\n",
    "        print filename, \" has dates out of range\"\n",
    "        print timeSeries.index\n",
    "    \n",
    "    return timeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOneModelAllFiles(dataDir,fileList,field,bounds,fileNameSplitter='_',fileNamePos=2,ensMemNumPos=4):\n",
    "    justFileName = fileList[0]\n",
    "    justModelName = str.split(justFileName,fileNameSplitter)[fileNamePos]\n",
    "    weightFileName = ''.join([weightPathAndPrefix,justModelName,weightSuffix]) #see constants.py\n",
    "    landFracFileName = None\n",
    "    if excludeOcean:\n",
    "        try:\n",
    "            landFracFileName = landFracFiles[justModelName][0]\n",
    "        except:\n",
    "            print \"no land frac file for \", justModelName\n",
    "    \n",
    "    ensMemList = []\n",
    "    for filename in fileList:\n",
    "        ensMemNum = str.split(filename,fileNameSplitter)[ensMemNumPos]\n",
    "        ensMemNum = str.split(ensMemNum,'i')[0]\n",
    "        ensMemNum = np.int(str.split(ensMemNum,'r')[1]) #the one or two digit integer between r and i in _r?i?p?_\n",
    "        if ensMemNum not in ensMemList:\n",
    "            ensMemList.append(ensMemNum)\n",
    "    \n",
    "    ensMemList.sort()\n",
    "    #print ensMemList\n",
    "    \n",
    "    def getOneEnsembleMember(ensMem):\n",
    "        firstFile = True\n",
    "        for filename in fileList:\n",
    "            ensMemNum = str.split(filename,fileNameSplitter)[ensMemNumPos]\n",
    "            ensMemNum = str.split(ensMemNum,'i')[0]\n",
    "            ensMemNum = np.int(str.split(ensMemNum,'r')[1]) #the one or two digit integer between r and i in _r?i?p?_\n",
    "\n",
    "            if ensMemNum == ensMem:\n",
    "                #print filename\n",
    "                onePartOfTimeSeries = getOneFile(''.join([dataDir,filename]),field,bounds,weightFileName,landFracFileName)\n",
    "                onePartOfTimeSeries.name = ensMemNum\n",
    "                if firstFile:\n",
    "                    timeSeries = onePartOfTimeSeries\n",
    "                    firstFile = False\n",
    "                else:\n",
    "                    timeSeries = pd.concat([timeSeries,onePartOfTimeSeries],axis=0)\n",
    "\n",
    "        timeSeries.name = ''.join(['run',str(ensMem)])\n",
    "        #print timeSeries.name\n",
    "        #--------------\n",
    "        to_return = pd.DataFrame(timeSeries)\n",
    "    \n",
    "        #This last bit is necessary because at least one file has duplicate rows, probably from ncrcatting together\n",
    "        #more than one version of the same file. The values in the duplicate rows are not all identical.\n",
    "        to_return = to_return.reset_index().drop_duplicates(subset='time',keep='last').set_index('time')\n",
    "        to_return = to_return.sort_index()\n",
    "\n",
    "        return to_return.transpose()\n",
    "    \n",
    "    timeSeriesModel = pd.concat([getOneEnsembleMember(ensMem) for ensMem in ensMemList], axis=0)\n",
    "    timeSeriesModel.name = justModelName\n",
    "        \n",
    "    return timeSeriesModel.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do all regions, using pr or tas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rcp85 pr BC\n",
      "BC pr rcp85 took 48.7498941422\n",
      "rcp85 pr global\n",
      "global pr rcp85 took 192.558516026\n",
      "rcp85 pr Alaska\n",
      "Alaska pr rcp85 took 12.3878419399\n",
      "rcp85 pr pnw\n",
      "pnw pr rcp85 took 11.6827740669\n",
      "rcp85 pr Cali\n",
      "Cali pr rcp85 took 11.7291791439\n",
      "rcp85 pr Baja\n",
      "Baja pr rcp85 took 11.6688759327\n",
      "rcp85 tas BC\n",
      "BC tas rcp85 took 184.889055967\n",
      "rcp85 tas global\n",
      "global tas rcp85 took 402.657424927\n",
      "rcp85 tas Alaska\n",
      "Alaska tas rcp85 took 176.079195976\n",
      "rcp85 tas pnw\n",
      "pnw tas rcp85 took 196.536866903\n",
      "rcp85 tas Cali\n",
      "Cali tas rcp85 took 195.897084951\n",
      "rcp85 tas Baja\n",
      "Baja tas rcp85 took 214.856924057\n",
      "rcp45 pr BC\n",
      "BC pr rcp45 took 31.1928248405\n",
      "rcp45 pr global\n",
      "global pr rcp45 took 274.597275972\n",
      "rcp45 pr Alaska\n",
      "Alaska pr rcp45 took 19.2081749439\n",
      "rcp45 pr pnw\n",
      "pnw pr rcp45 took 16.6276929379\n",
      "rcp45 pr Cali\n",
      "Cali pr rcp45 took 18.9855501652\n",
      "rcp45 pr Baja\n",
      "Baja pr rcp45 took 25.0149991512\n",
      "rcp45 tas BC\n",
      "BC tas rcp45 took 177.816021919\n",
      "rcp45 tas global\n",
      "global tas rcp45 took 388.370023012\n",
      "rcp45 tas Alaska\n",
      "Alaska tas rcp45 took 235.889532089\n",
      "rcp45 tas pnw\n",
      "pnw tas rcp45 took 173.020188808\n",
      "rcp45 tas Cali\n",
      "Cali tas rcp45 took 173.529597998\n",
      "rcp45 tas Baja\n",
      "Baja tas rcp45 took 171.605571985\n",
      "rcp60 pr BC\n",
      "BC pr rcp60 took 98.3467469215\n",
      "rcp60 pr global\n",
      "global pr rcp60 took 98.5638329983\n",
      "rcp60 pr Alaska\n",
      "Alaska pr rcp60 took 4.21311402321\n",
      "rcp60 pr pnw\n",
      "pnw pr rcp60 took 3.85397791862\n",
      "rcp60 pr Cali\n",
      "Cali pr rcp60 took 3.90826296806\n",
      "rcp60 pr Baja\n",
      "Baja pr rcp60 took 3.80094408989\n",
      "rcp60 tas BC\n",
      "BC tas rcp60 took 179.23098588\n",
      "rcp60 tas global\n",
      "global tas rcp60 took 191.207386017\n",
      "rcp60 tas Alaska\n",
      "Alaska tas rcp60 took 86.8901581764\n",
      "rcp60 tas pnw\n",
      "pnw tas rcp60 took 81.9491138458\n",
      "rcp60 tas Cali\n",
      "Cali tas rcp60 took 79.7023789883\n",
      "rcp60 tas Baja\n",
      "Baja tas rcp60 took 85.5806350708\n",
      "historical pr BC\n",
      "BC pr historical took 616.647900105\n",
      "historical pr global\n",
      "global pr historical took 649.031883955\n",
      "historical pr Alaska\n",
      "Alaska pr historical took 145.252931833\n",
      "historical pr pnw\n",
      "pnw pr historical took 213.71827507\n",
      "historical pr Cali\n",
      "Cali pr historical took 221.179522038\n",
      "historical pr Baja\n",
      "Baja pr historical took 224.217112064\n",
      "historical tas BC\n",
      "BC tas historical took 1056.45561004\n",
      "historical tas global\n",
      "global tas historical took 993.899834871\n",
      "historical tas Alaska\n",
      "Alaska tas historical took 493.196581841\n",
      "historical tas pnw\n",
      "pnw tas historical took 501.438371897\n",
      "historical tas Cali\n",
      "Cali tas historical took 487.07716012\n",
      "historical tas Baja\n",
      "Baja tas historical took 602.152902842\n",
      "rcp26 pr BC\n",
      "BC pr rcp26 took 134.404344082\n",
      "rcp26 pr global\n",
      "global pr rcp26 took 140.701107979\n",
      "rcp26 pr Alaska\n",
      "Alaska pr rcp26 took 7.20857214928\n",
      "rcp26 pr pnw\n",
      "pnw pr rcp26 took 5.51028800011\n",
      "rcp26 pr Cali\n",
      "Cali pr rcp26 took 5.59104013443\n",
      "rcp26 pr Baja\n",
      "Baja pr rcp26 took 5.43890500069\n",
      "rcp26 tas BC\n",
      "BC tas rcp26 took 267.175523996\n",
      "rcp26 tas global\n",
      "global tas rcp26 took 250.221518993\n",
      "rcp26 tas Alaska\n",
      "Alaska tas rcp26 took 103.614464998\n",
      "rcp26 tas pnw\n",
      "pnw tas rcp26 took 111.591801882\n",
      "rcp26 tas Cali\n",
      "Cali tas rcp26 took 90.0907199383\n",
      "rcp26 tas Baja\n",
      "Baja tas rcp26 took 82.3357100487\n"
     ]
    }
   ],
   "source": [
    "scenariosCMIP = ['rcp85','rcp45','rcp60','historical','rcp26']\n",
    "fields = ['pr','tas']\n",
    "excludeOcean = False\n",
    "\n",
    "landFracFiles = getFileInfo(landFracPath)\n",
    "\n",
    "for scenario in scenariosCMIP:\n",
    "    for field in fields:\n",
    "        # edit dataDir to reflect your directory structure\n",
    "        dataDir = ''.join([dataDirCMIP,scenario,'/',field,'/'])\n",
    "        fileDict = getFileInfo(dataDir)\n",
    "        if excludeOcean:\n",
    "            # leave if off if there is no land-sea mask:\n",
    "            for key in fileDict.keys():\n",
    "                if key not in landFracFiles.keys():\n",
    "                    del fileDict[key]\n",
    "        \n",
    "        # regionBounds is a dictionary defined as a global variable in constants.py\n",
    "        for regionKey in regionBounds:\n",
    "            print scenario, field, regionKey\n",
    "            time0 = time()\n",
    "            allData = pd.DataFrame()\n",
    "            allData = pd.concat([getOneModelAllFiles(dataDir,fileDict[model],field,regionBounds[regionKey]).transpose() for model in fileDict.keys()],axis=0,keys=fileDict.keys())\n",
    "            indexToUse = pd.MultiIndex.from_tuples(allData.transpose(),names=['model','run'])\n",
    "            allData.index = indexToUse\n",
    "            allData = allData.transpose()\n",
    "            if scenario == 'historical':\n",
    "                allData = allData['185001':'200512']\n",
    "            outFile = ''.join(['timeSeries/timeSeries_',field,'_',regionKey,'_Monthly_',scenario,'.csv'])\n",
    "            allData.to_csv(outFile)\n",
    "            time1 = time()\n",
    "            print ' '.join([regionKey,field,scenario,'took',str(time1-time0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
