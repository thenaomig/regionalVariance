{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the definitive version for producing CMIP5 time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xy\n",
    "import string\n",
    "from copy import deepcopy\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "from constants import * #<= my defined global variables\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFileInfo(dataDir,fileNameSplitter='_',fileNamePos=2):\n",
    "    def justDataFiles(path=None):\n",
    "        for f in os.listdir(path):\n",
    "            if ('p1_21' not in f) and ('p1_22' not in f) and ('.nc' in f):\n",
    "                yield f\n",
    "            \n",
    "    files = [x for x in justDataFiles(dataDir)]  \n",
    "    files.sort()\n",
    "    \n",
    "    #make a list of the model names in this file list\n",
    "    modelNames = []\n",
    "    for oneFile in files:\n",
    "        justFileName = str.split(oneFile,'/')[-1]\n",
    "        #print justFileName\n",
    "        justEnsMemNum = str.split(justFileName,fileNameSplitter)[fileNamePos]\n",
    "        #print justEnsMemNum\n",
    "        modelNames.append(justEnsMemNum)\n",
    "    modelNames = list(set(modelNames))\n",
    "\n",
    "    #make a dictionary of all the files that go with each model\n",
    "    keyvaluepairs = []\n",
    "    for model in modelNames:\n",
    "        uniqueModelName = ''.join([model,fileNameSplitter])\n",
    "        oneModelFileList = []\n",
    "        for oneFile in files:\n",
    "            if uniqueModelName in oneFile:\n",
    "                oneModelFileList.append(oneFile)\n",
    "        tupleOfKeyValue = (model,oneModelFileList)\n",
    "        keyvaluepairs.append(tupleOfKeyValue)\n",
    "\n",
    "    fileDict = dict(keyvaluepairs)\n",
    "    \n",
    "    return fileDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAreaAvg(data,bounds,weights,landFrac): #,t,weights):\n",
    "\n",
    "    weightedMean = 0.0\n",
    "    weightTotal = 0.0\n",
    "    \n",
    "    areaSubset = data.sel(lon=slice(bounds.lonMin,bounds.lonMax),lat=slice(bounds.latMin,bounds.latMax)) #.isel(time=t)\n",
    "    if landFrac and excludeOcean:\n",
    "        landFrac = landFrac.sel(lon=slice(bounds.lonMin,bounds.lonMax),lat=slice(bounds.latMin,bounds.latMax))\n",
    "        areaSubset = areaSubset.where(landFrac>0.5)\n",
    "        \n",
    "    useCosLat = False\n",
    "    if weights is None:\n",
    "        useCosLat = True\n",
    "    else:\n",
    "        try:\n",
    "            if len(weights.dims)==2:\n",
    "                weightsSubset = weights.sel(lon=slice(bounds.lonMin,bounds.lonMax),lat=slice(bounds.latMin,bounds.latMax))\n",
    "                weightedMean = (areaSubset * np.asarray(weightsSubset)).sum('lat').sum('lon')\n",
    "            elif len(weights.dims)==1:\n",
    "                weightsSubset = weights.sel(lat=slice(bounds.latMin,bounds.latMax))\n",
    "                weightedMean = (areaSubset.mean('lon') * np.asarray(weightsSubset)).sum('lat')\n",
    "        except:\n",
    "            #try *not* using the weights from the files, which might need to be updated\n",
    "            useCosLat = True\n",
    "            print \"an exception!\", weights.shape\n",
    "            \n",
    "    if useCosLat:\n",
    "        weightsSubset = np.cos(np.deg2rad(areaSubset.lat))\n",
    "        weightedMean = (areaSubset.mean('lon') * np.asarray(weightsSubset)).sum('lat')\n",
    "        \n",
    "    weightTotal = weightsSubset.sum()\n",
    "    return weightedMean/np.double(weightTotal)\n",
    "\n",
    "def convertPrecipUnits(dataVsTime, startingFrom=''): # converts to mm/day                                                                                                                       \n",
    "    if startingFrom == 'm/s':\n",
    "        return dataVsTime * 1000.0 * 60.0 * 60.0 * 24.0\n",
    "    else: #default for CMIP5                                                                                                                           \n",
    "        return dataVsTime * 60.0*60.0*24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOneFile(filename,field,bounds,weightFileName,landFracFileName):\n",
    "    \n",
    "    oneEnsMem = xy.open_dataset(filename)\n",
    "        \n",
    "    #get weights from weighta file if exists:\n",
    "    try:\n",
    "        weights = xy.open_dataset(weightFileName)\n",
    "        weights = weights.areacella\n",
    "    except:\n",
    "        weights = None\n",
    "        \n",
    "    if excludeOcean:\n",
    "        #get land fraction data if exists:\n",
    "        try:\n",
    "            landFrac = xy.open_dataset(landFracFileName)\n",
    "            landFrac = landFrac.sftlf\n",
    "        except:\n",
    "            landFrac = None\n",
    "    else:\n",
    "        landFrac = None\n",
    "            \n",
    "    #we're not looking out past 2100, which a few models provide, and anyway it would cause indexing errors\n",
    "    if ('historical' not in filename) and ('CESMens' not in filename):\n",
    "        if ('IPSL-CM5A-LR' in filename or 'CSIRO-Mk3L-1-2' in filename): #len(oneEnsMem.time) >= 2262:\n",
    "            oneEnsMem = oneEnsMem.isel(time=slice(0,1140)) #just until 2100-12 century\n",
    "            oneEnsMem['time'] = pd.date_range('2006-01-01',periods=1140,freq='MS')\n",
    "        oneEnsMem = oneEnsMem.sel(time=slice('2006-01-01','2099-12-31')) #a bit more exactly\n",
    "        #if(np.bool(oneEnsMem.time[-1] < np.datetime64('2099-11-30'))):\n",
    "            #print \"file doesn't go to end-of-century, only \", np.datetime_as_string(oneEnsMem.time[-1])\n",
    "    \n",
    "    timeSeries = getAreaAvg(oneEnsMem[field],bounds,weights,landFrac).to_series() \n",
    "    if field == 'tas':\n",
    "        if oneEnsMem[field].units != 'K':\n",
    "            print filename\n",
    "            print oneEnsMem[field].units\n",
    "        if np.double(oneEnsMem[field].mean()) < 200.0:\n",
    "            print filename\n",
    "            print oneEnsMem[field].units\n",
    "    \n",
    "    if field == 'pr':\n",
    "        timeSeries = convertPrecipUnits(timeSeries)\n",
    "    \n",
    "    try:    \n",
    "        timeSeries.index = timeSeries.index.to_period(freq='M') \n",
    "    except:\n",
    "        print filename, \" has dates out of range\"\n",
    "        print timeSeries.index\n",
    "    \n",
    "    return timeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getOneModelAllFiles(dataDir,fileList,field,bounds,fileNameSplitter='_',fileNamePos=2,ensMemNumPos=4):\n",
    "    justFileName = fileList[0]\n",
    "    justModelName = str.split(justFileName,fileNameSplitter)[fileNamePos]\n",
    "    weightFileName = ''.join([weightPathAndPrefix,justModelName,weightSuffix]) #see constants.py\n",
    "    landFracFileName = None\n",
    "    if excludeOcean:\n",
    "        try:\n",
    "            landFracFileName = landFracFiles[justModelName][0]\n",
    "        except:\n",
    "            print \"no land frac file for \", justModelName\n",
    "    \n",
    "    ensMemList = []\n",
    "    for filename in fileList:\n",
    "        ensMemNum = str.split(filename,fileNameSplitter)[ensMemNumPos]\n",
    "        ensMemNum = str.split(ensMemNum,'i')[0]\n",
    "        ensMemNum = np.int(str.split(ensMemNum,'r')[1]) #the one or two digit integer between r and i in _r?i?p?_\n",
    "        if ensMemNum not in ensMemList:\n",
    "            ensMemList.append(ensMemNum)\n",
    "    \n",
    "    ensMemList.sort()\n",
    "    #print ensMemList\n",
    "    \n",
    "    def getOneEnsembleMember(ensMem):\n",
    "        firstFile = True\n",
    "        for filename in fileList:\n",
    "            ensMemNum = str.split(filename,fileNameSplitter)[ensMemNumPos]\n",
    "            ensMemNum = str.split(ensMemNum,'i')[0]\n",
    "            ensMemNum = np.int(str.split(ensMemNum,'r')[1]) #the one or two digit integer between r and i in _r?i?p?_\n",
    "\n",
    "            if ensMemNum == ensMem:\n",
    "                #print filename\n",
    "                onePartOfTimeSeries = getOneFile(''.join([dataDir,filename]),field,bounds,weightFileName,landFracFileName)\n",
    "                onePartOfTimeSeries.name = ensMemNum\n",
    "                if firstFile:\n",
    "                    timeSeries = onePartOfTimeSeries\n",
    "                    firstFile = False\n",
    "                else:\n",
    "                    timeSeries = pd.concat([timeSeries,onePartOfTimeSeries],axis=0)\n",
    "\n",
    "        timeSeries.name = ''.join(['run',str(ensMem)])\n",
    "        #print timeSeries.name\n",
    "        #--------------\n",
    "        to_return = pd.DataFrame(timeSeries)\n",
    "    \n",
    "        #This last bit is necessary because at least one file has duplicate rows, probably from ncrcatting together\n",
    "        #more than one version of the same file. The values in the duplicate rows are not all identical.\n",
    "        to_return = to_return.reset_index().drop_duplicates(subset='time',keep='last').set_index('time')\n",
    "        to_return = to_return.sort_index()\n",
    "\n",
    "        return to_return.transpose()\n",
    "    \n",
    "    timeSeriesModel = pd.concat([getOneEnsembleMember(ensMem) for ensMem in ensMemList], axis=0)\n",
    "    timeSeriesModel.name = justModelName\n",
    "        \n",
    "    return timeSeriesModel.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do all regions, using pr or tas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disk/grease/naomi/anaconda/lib/python2.7/site-packages/xarray/conventions.py:393: RuntimeWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using dummy netCDF4.datetime objects instead, reason: dates out of range\n",
      "  result = decode_cf_datetime(example_value, units, calendar)\n",
      "/home/disk/grease/naomi/anaconda/lib/python2.7/site-packages/xarray/conventions.py:412: RuntimeWarning: Unable to decode time axis into full numpy.datetime64 objects, continuing using dummy netCDF4.datetime objects instead, reason: dates out of range\n",
      "  calendar=self.calendar)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rcp85 pr BC\n",
      "BC pr rcp85 took 91.6717500687\n",
      "rcp85 pr global\n",
      "global pr rcp85 took 268.291644096\n",
      "rcp85 pr Alaska\n",
      "Alaska pr rcp85 took 62.1464021206\n",
      "rcp85 pr pnw\n",
      "pnw pr rcp85 took 56.437194109\n",
      "rcp85 pr Cali\n",
      "Cali pr rcp85 took 51.7437930107\n",
      "rcp85 pr Baja\n",
      "Baja pr rcp85 took 80.6927351952\n",
      "rcp85 tas BC\n",
      "BC tas rcp85 took 364.333054066\n",
      "rcp85 tas global\n",
      "global tas rcp85 took 500.049427986\n",
      "rcp85 tas Alaska\n",
      "Alaska tas rcp85 took 206.385894775\n",
      "rcp85 tas pnw\n",
      "pnw tas rcp85 took 197.864928961\n",
      "rcp85 tas Cali\n",
      "Cali tas rcp85 took 208.137690067\n",
      "rcp85 tas Baja\n",
      "Baja tas rcp85 took 230.155840874\n",
      "rcp45 pr BC\n",
      "BC pr rcp45 took 186.089550972\n",
      "rcp45 pr global\n",
      "global pr rcp45 took 319.948368073\n",
      "rcp45 pr Alaska\n",
      "Alaska pr rcp45 took 16.2066869736\n",
      "rcp45 pr pnw\n",
      "pnw pr rcp45 took 15.1619939804\n",
      "rcp45 pr Cali\n",
      "Cali pr rcp45 took 15.2247130871\n",
      "rcp45 pr Baja\n",
      "Baja pr rcp45 took 15.0078449249\n",
      "rcp45 tas BC\n",
      "BC tas rcp45 took 479.560211897\n",
      "rcp45 tas global\n",
      "global tas rcp45 took 496.964792013\n",
      "rcp45 tas Alaska\n",
      "Alaska tas rcp45 took 170.289685965\n",
      "rcp45 tas pnw\n",
      "pnw tas rcp45 took 154.743723869\n",
      "rcp45 tas Cali\n",
      "Cali tas rcp45 took 141.906105042\n",
      "rcp45 tas Baja\n",
      "Baja tas rcp45 took 140.938766956\n",
      "rcp60 pr BC\n",
      "BC pr rcp60 took 55.222244978\n",
      "rcp60 pr global\n",
      "global pr rcp60 took 86.4079871178\n",
      "rcp60 pr Alaska\n",
      "Alaska pr rcp60 took 3.48081302643\n",
      "rcp60 pr pnw\n",
      "pnw pr rcp60 took 3.33172702789\n",
      "rcp60 pr Cali\n",
      "Cali pr rcp60 took 3.33193993568\n",
      "rcp60 pr Baja\n",
      "Baja pr rcp60 took 3.3985850811\n",
      "rcp60 tas BC\n",
      "BC tas rcp60 took 132.323551178\n",
      "rcp60 tas global\n",
      "global tas rcp60 took 224.546755075\n",
      "rcp60 tas Alaska\n",
      "Alaska tas rcp60 took 84.9890367985\n",
      "rcp60 tas pnw\n",
      "pnw tas rcp60 took 85.9577448368\n",
      "rcp60 tas Cali\n",
      "Cali tas rcp60 took 75.4328780174\n",
      "rcp60 tas Baja\n",
      "Baja tas rcp60 took 75.8066718578\n",
      "historical pr BC\n",
      "BC pr historical took 416.710672855\n",
      "historical pr global\n",
      "global pr historical took 545.942071915\n",
      "historical pr Alaska\n",
      "Alaska pr historical took 47.1776740551\n",
      "historical pr pnw\n",
      "pnw pr historical took 37.4855611324\n",
      "historical pr Cali\n",
      "Cali pr historical took 37.1336112022\n",
      "historical pr Baja\n",
      "Baja pr historical took 37.0569951534\n",
      "historical tas BC\n",
      "BC tas historical took 816.31379509\n",
      "historical tas global\n",
      "global tas historical took 937.149254084\n",
      "historical tas Alaska\n",
      "Alaska tas historical took 705.5172019\n",
      "historical tas pnw\n",
      "pnw tas historical took 475.451425076\n",
      "historical tas Cali\n",
      "Cali tas historical took 415.447883129\n",
      "historical tas Baja\n",
      "Baja tas historical took 409.847355127\n",
      "rcp26 pr BC\n",
      "BC pr rcp26 took 125.94492197\n",
      "rcp26 pr global\n",
      "global pr rcp26 took 157.154314995\n",
      "rcp26 pr Alaska\n",
      "Alaska pr rcp26 took 5.36594796181\n",
      "rcp26 pr pnw\n",
      "pnw pr rcp26 took 4.83466100693\n",
      "rcp26 pr Cali\n",
      "Cali pr rcp26 took 4.90791416168\n",
      "rcp26 pr Baja\n",
      "Baja pr rcp26 took 4.8757879734\n",
      "rcp26 tas BC\n",
      "BC tas rcp26 took 224.009351969\n",
      "rcp26 tas global\n",
      "global tas rcp26 took 227.408682108\n",
      "rcp26 tas Alaska\n",
      "Alaska tas rcp26 took 95.8397448063\n",
      "rcp26 tas pnw\n",
      "pnw tas rcp26 took 90.2373671532\n",
      "rcp26 tas Cali\n",
      "Cali tas rcp26 took 105.630174875\n",
      "rcp26 tas Baja\n",
      "Baja tas rcp26 took 96.2358949184\n"
     ]
    }
   ],
   "source": [
    "scenariosCMIP = ['rcp85','rcp45','rcp60','historical','rcp26']\n",
    "fields = ['pr','tas']\n",
    "excludeOcean = True\n",
    "\n",
    "landFracFiles = getFileInfo(landFracPath)\n",
    "\n",
    "for scenario in scenariosCMIP:\n",
    "    for field in fields:\n",
    "        # edit dataDir to reflect your directory structure\n",
    "        dataDir = ''.join([dataDirCMIP,scenario,'/',field,'/'])\n",
    "        fileDict = getFileInfo(dataDir)\n",
    "        if excludeOcean:\n",
    "            # leave if off if there is no land-sea mask:\n",
    "            for key in fileDict.keys():\n",
    "                if key not in landFracFiles.keys():\n",
    "                    del fileDict[key]\n",
    "        \n",
    "        # regionBounds is a dictionary defined as a global variable in constants.py\n",
    "        for regionKey in regionBounds:\n",
    "            if regionKey != 'global':\n",
    "                excludeOcean = True\n",
    "            else:\n",
    "                excludeOcean = False\n",
    "            print scenario, field, regionKey\n",
    "            time0 = time()\n",
    "            allData = pd.DataFrame()\n",
    "            allData = pd.concat([getOneModelAllFiles(dataDir,fileDict[model],field,regionBounds[regionKey]).transpose() for model in fileDict.keys()],axis=0,keys=fileDict.keys())\n",
    "            indexToUse = pd.MultiIndex.from_tuples(allData.transpose(),names=['model','run'])\n",
    "            allData.index = indexToUse\n",
    "            allData = allData.transpose()\n",
    "            if scenario == 'historical':\n",
    "                allData = allData['185001':'200512']\n",
    "            if excludeOcean:\n",
    "                outFile = ''.join(['timeSeries/timeSeriesLO_',field,'_',regionKey,'_Monthly_',scenario,'.csv'])\n",
    "            else:\n",
    "                outFile = ''.join(['timeSeries/timeSeries_',field,'_',regionKey,'_Monthly_',scenario,'.csv'])\n",
    "            allData.to_csv(outFile)\n",
    "            time1 = time()\n",
    "            print ' '.join([regionKey,field,scenario,'took',str(time1-time0)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
